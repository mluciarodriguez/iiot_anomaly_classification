{"cells":[{"cell_type":"markdown","source":["#ANOMALY CLASSIFICATION IN IIoT SYSTEMS"],"metadata":{"id":"kAzpH15Y9meh"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"t6MH3VTWYuG6","executionInfo":{"status":"ok","timestamp":1718576082179,"user_tz":300,"elapsed":2893,"user":{"displayName":"Lucia Rodriguez","userId":"04260983339017832132"}}},"outputs":[],"source":["import numpy as np\n","import os\n","import tensorflow as tf\n","import pandas as pd\n","from tensorflow import keras\n","from keras import layers\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPYmh40vhy-s"},"outputs":[],"source":["# Detect hardware\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","  print(f'Running on a TPU w/{tpu.num_accelerators()[\"TPU\"]} cores')\n","except ValueError:\n","  raise BaseException('ERROR: Not connected to a TPU runtime;')\n","tf.config.experimental_connect_to_cluster(tpu)\n","tf.tpu.experimental.initialize_tpu_system(tpu)\n","tpu_strategy = tf.distribute.TPUStrategy(tpu)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E6WD_lQfZZtx"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHy9E-MuZpFO"},"outputs":[],"source":["pd.options.mode.copy_on_write = True #to save on memory and avoid making copies of data until written to\n","df = pd.read_csv('/content/drive/MyDrive/Colab/UdeA/normalized/edge1/edge1_11.csv')\n","df.reindex(['localtime', 'indoor_temp', 'indoor_humidity', 'edge_soc_temp', 'edge_load1', 'edge_load15',\n","          'delta_sent', 'delta_recv', 'edge_current', 'motor_sound', 'motor_freq', 'motor_volt',\n","          'motor_current', 'motor_recipe','edge_cpu_freq', 'edge_load5','edge_cpu_voltage', 'edge_memory_free',\n","          'edge_memory_avail',  'edge_wifi_send', 'edge_wifi_receiv', 'fault_type', 'attack_type', 'anomaly',\n","          'normal', 'E', 'A', 'F', 'N'], axis=1)\n","x_train=df.iloc[:,3:11]\n","y_train = df.iloc[:,-3:]\n","print('y_train: ',y_train.shape)\n","print('x_train: ',x_train.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"wu1c6V68Uc8H","executionInfo":{"status":"ok","timestamp":1718576194621,"user_tz":300,"elapsed":192,"user":{"displayName":"Lucia Rodriguez","userId":"04260983339017832132"}}},"outputs":[],"source":["# Generate training sequence\n","def create_sequences(values, n_samples, n_times, n_features):\n","  output = np.zeros( (n_samples-n_times, n_times, n_features) )\n","  for j in range(np.array(n_features)):\n","    features = []\n","    for i in range(np.array(n_samples - n_times)):\n","      output[i,:,j]=values[i : (i + n_times),j]\n","  return output"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"AR4OaWvZUSRf","executionInfo":{"status":"ok","timestamp":1718576196103,"user_tz":300,"elapsed":182,"user":{"displayName":"Lucia Rodriguez","userId":"04260983339017832132"}}},"outputs":[],"source":["N_TIMES = 300"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PF2pj7FYUeR9"},"outputs":[],"source":["# create_sequences(df_training, N_SAMPLES, N_TIMES, N_FEATURES) for x_train\n","df_training = np.array(x_train[:])\n","N_SAMPLES= df_training.shape[0]\n","N_FEATURES = df_training.shape[1]\n","x_train_sequence= create_sequences(np.array(df_training), np.array(N_SAMPLES), np.array(N_TIMES), np.array(N_FEATURES))\n","x_train_sequence = x_train_sequence.astype(np.float32)\n","x_train_sequence.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X5OqbCkEUhxM"},"outputs":[],"source":["# create_sequences(df_training, N_SAMPLES, N_TIMES, N_FEATURES) for y_train\n","df_training = np.array(y_train[:])\n","N_SAMPLES= df_training.shape[0]\n","N_FEATURES = df_training.shape[1]\n","y_train_sequence= create_sequences(df_training, N_SAMPLES, N_TIMES, N_FEATURES)\n","y_train_sequence = y_train_sequence.astype(np.float32)\n","y_train_sequence.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fm0TIoyBDsh-"},"outputs":[],"source":["import random\n","#BALANCE USING SEQUENCE ON OUTPUT\n","secuencia = np.sum(y_train_sequence, axis=1)\n","lista = list(sum(secuencia)/N_TIMES)\n","if lista[0] > lista[1]:\n","  umbral = int(lista[2] - lista[0])\n","else:\n","  umbral = int(lista[2] - lista[1])\n","#'umbral' has the amount of normal data that exceeds the abnormal data\n","print(umbral)\n","indices=list(np.where((secuencia[:, 2]> secuencia[:, 1]) & (secuencia[:, 2]> secuencia[:, 0]))[0])\n","posiciones = sorted(random.sample(indices, umbral))\n","#Eliminate these rows:\n","y_train_sequence = np.delete(y_train_sequence, posiciones, axis=0)\n","x_train_sequence = np.delete(x_train_sequence, posiciones, axis=0)\n","print(y_train_sequence.shape)\n","print(x_train_sequence.shape)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"DIwCLvdSRkhh","executionInfo":{"status":"ok","timestamp":1718576204311,"user_tz":300,"elapsed":933,"user":{"displayName":"Lucia Rodriguez","userId":"04260983339017832132"}}},"outputs":[],"source":["from keras.layers import MultiHeadAttention, Input, Dense, Softmax, Conv1D, Dropout, GlobalAveragePooling1D\n","from keras.layers import LayerNormalization, Layer\n","from keras.layers import TextVectorization, Embedding\n","from tensorflow import convert_to_tensor\n","from keras import Model, Sequential\n","import math\n","from sklearn import metrics\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"OZzGcf7-xQuz","executionInfo":{"status":"ok","timestamp":1718576205908,"user_tz":300,"elapsed":5,"user":{"displayName":"Lucia Rodriguez","userId":"04260983339017832132"}}},"outputs":[],"source":["# https://colab.research.google.com/drive/1lru2zGbF3kB5N5Blu2ZXh-9Z8sxE3SSb#scrollTo=1H5Wk08rWKAj\n","# total_heads: How many Attention units are we using?\n","# dense_units:  Number of neurons in dense layer with relu-activation (you can try different values)\n","# embed_dim: x_train_sequence.shape[2] how many variables are there in dataset?\n","# sequence_length: x_train_sequence.shape[1] samples in time-window\n","# classes: 1: Normal - Failure - Attack\n","\n","keras.saving.get_custom_objects().clear()\n","@keras.saving.register_keras_serializable(package=\"MyLayers\")\n","class EncoderLayer(tf.keras.Model):\n","    def __init__(self, total_heads, dense_units, sequence_length, embed_dim, classes, **kwargs):\n","        super(EncoderLayer, self).__init__(**kwargs)\n","        self.total_heads = total_heads\n","        self.dense_units = dense_units\n","        self.sequence_length = sequence_length\n","        self.embed_dim = embed_dim\n","        self.classes = classes\n","        self.multihead = MultiHeadAttention(num_heads=total_heads, key_dim=embed_dim)\n","        self.forward = Sequential([Dense(self.embed_dim)])\n","        self.normalize_layer = LayerNormalization()\n","\n","    def call(self, x):\n","        multihead_layer = self.multihead(x,x)\n","        normalize1_layer = self.normalize_layer(x + multihead_layer)\n","        forward_layer = self.forward(normalize1_layer)\n","        normalize2_layer = self.normalize_layer(normalize1_layer + forward_layer)\n","        final_output = self.forward(normalize2_layer)\n","        return final_output\n","\n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {\"total_heads\": self.total_heads,\"dense_units\": self.dense_units,\n","                \"sequence_length\": self.sequence_length,\"embed_dim\": self.embed_dim,\n","                \"classes\": self.classes, **base_config}\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uUtk8VckXatX"},"outputs":[],"source":["# Construct the transformer model\n","with tpu_strategy.scope():\n","  total_heads=20\n","  sequence_length = x_train_sequence.shape[1] #timestep\n","  embed_dim = x_train_sequence.shape[2]       #variables in dataset\n","  classes = y_train_sequence.shape[2]         # A-F-N\n","  dense_units = embed_dim\n","  # Custom layers\n","  encoder_layer = EncoderLayer(total_heads, dense_units, sequence_length, embed_dim, classes)\n","  classify_layer = Dense(classes, activation='softmax')\n","  # Start connecting the layers together\n","  inputs = Input(shape=(sequence_length,embed_dim ))\n","  encoder_1 = encoder_layer(inputs) #directly assign x without embedding\n","  encoder_2 = encoder_layer(encoder_1)\n","  encoder_3 = encoder_layer(encoder_2)\n","  dense = Dense(dense_units, activation=\"relu\")(encoder_3)\n","  outputs = classify_layer(dense)\n","  model = Model(inputs=inputs, outputs=outputs)\n","\n","  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy')\n","  model.save(\"/content/drive/MyDrive/Colab/models/transformer.keras\")\n","model.summary()"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"pH64doCEsmS2","executionInfo":{"status":"ok","timestamp":1718576250514,"user_tz":300,"elapsed":220,"user":{"displayName":"Lucia Rodriguez","userId":"04260983339017832132"}}},"outputs":[],"source":["files_dic1 = {'/content/drive/MyDrive/Colab/UdeA/normalized/edge1/': ['edge1_11.csv', 'edge1_12.csv', 'edge1_13.csv', 'edge1_14.csv']}\n","files_dic2 = {'/content/drive/MyDrive/Colab/UdeA/normalized/edge2/': ['edge2_9.csv','edge2_10.csv', 'edge2_11.csv', 'edge2_12.csv', 'edge2_13.csv']}\n","files_dic3 = {'/content/drive/MyDrive/Colab/UdeA/normalized/edge3/': ['edge3_8.csv', 'edge3_9.csv', 'edge3_10.csv', 'edge3_11.csv', 'edge3_12.csv']}\n","files_dic4 = {'/content/drive/MyDrive/Colab/UdeA/normalized/edge4/': ['edge4_10.csv','edge4_11.csv', 'edge4_12.csv']}\n","files_dic = [files_dic1, files_dic2, files_dic3, files_dic4]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qMbebhUYsq3N"},"outputs":[],"source":["from pathlib import Path\n","datos =[]\n","for m in range(len(files_dic)):\n","  for key,value in files_dic[m].items():\n","    for file in value:\n","      my_file =key + file\n","      datos.append(my_file)\n","print(datos)\n","datos_val=datos[:]"]},{"cell_type":"markdown","source":["**Training the model with each csv file**\n","\n","As the model is stored in Google Drive, when the history is graphed, you can choose the version (epoch) with the best metrics and return to that version in Google."],"metadata":{"id":"oPndzkhtSkFj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Puz_aSFu_dZH"},"outputs":[],"source":["from pathlib import Path\n","import random\n","import json\n","import random\n","random.seed(123)\n","N_TIMES = 300\n","ciclos = 0\n","with tpu_strategy.scope():\n","  model = keras.models.load_model(\"/content/drive/MyDrive/Colab/models/transformer.keras\", compile=False)\n","  model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy')\n","for i in range(50):\n","  for jx in range(len(datos)):\n","    print(datos[jx])\n","    df_train = pd.read_csv(datos[jx])\n","    x_train = df_train[['edge_soc_temp', 'edge_load1', 'edge_load15', 'delta_sent', 'delta_recv', 'edge_current', 'motor_sound', 'motor_freq']]\n","    x_train = x_train.to_numpy(np.float32)\n","    y_train = df_train.iloc[:,-3:]\n","    y_train = y_train.to_numpy(np.float32)\n","\n","    with tpu_strategy.scope():\n","      #creating sequences\n","      N_SAMPLES= x_train.shape[0]\n","      N_FEATURES = x_train.shape[1]\n","      x_train_sequence= create_sequences(x_train, N_SAMPLES, N_TIMES, N_FEATURES)\n","      x_train_sequence = x_train_sequence.astype(np.float32)\n","      N_SAMPLES= y_train.shape[0]\n","      N_FEATURES = y_train.shape[1]\n","      y_train_sequence= create_sequences(y_train, N_SAMPLES, N_TIMES, N_FEATURES)\n","      y_train_sequence = y_train_sequence.astype(np.float32)\n","      secuencia = np.sum(y_train_sequence, axis=1)\n","      #eliminating normal sequences to balance data\n","      lista = list(sum(secuencia)/N_TIMES)\n","      if lista[0] > lista[1]:\n","        umbral = int(lista[2] - lista[0])\n","      else:\n","        umbral = int(lista[2] - lista[1])\n","      indices=list(np.where((secuencia[:, 2]> secuencia[:, 1]) & (secuencia[:, 2]> secuencia[:, 0]))[0])\n","      posiciones = sorted(random.sample(indices, umbral))\n","      y_train_sequence = np.delete(y_train_sequence, posiciones, axis=0)\n","      x_train_sequence = np.delete(x_train_sequence, posiciones, axis=0)\n","      history = model.fit(\n","          x_train_sequence,\n","          y_train_sequence,\n","          epochs = 1,\n","          batch_size=128,\n","          validation_batch_size=128,\n","          shuffle=False,\n","          callbacks=[keras.callbacks.EarlyStopping(monitor=\"loss\", patience=200, mode=\"min\")]\n","      )\n","    with tpu_strategy.scope():\n","      model.save(\"/content/drive/MyDrive/Colab/models/transformer.keras\")\n","\n","print(history.history)\n"]},{"cell_type":"code","source":["model = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab/models/transformer.keras\")\n","model.summary()"],"metadata":{"id":"K3M2YZctSCq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Graph history\n","import matplotlib.pyplot as plt\n","print(history.history.keys())\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"],"metadata":{"id":"Kk4A7bcTSCOU"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}